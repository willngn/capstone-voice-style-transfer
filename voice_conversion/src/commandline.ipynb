{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183500,"status":"ok","timestamp":1701575123339,"user":{"displayName":"Tam Nguyen","userId":"17807644483590929096"},"user_tz":-480},"id":"R7-eGN_m0_Ow","outputId":"c5af1857-98b5-40e5-ce52-72def942584f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(epoch=100, n_epochs=10, model_name='pretrained', dataset='database', n_spkrs=2, batch_size=4, lr=1e-10, b1=0.5, b2=0.999, decay_epoch=5, n_cpu=8, img_height=128, img_width=128, channels=1, plot_interval=-1, checkpoint_interval=1, n_downsample=2, dim=32)\n","/usr/local/lib/python3.10/dist-packages/torch/_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n","  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","  0% 0/36 [00:00<?, ?it/s]/content/drive/MyDrive/capstone-voice-style-transfer/voice_conversion/src/train.py:44: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  valid = Variable(Tensor(np.ones((X1.size(0), *D[id_1].output_shape))), requires_grad=False)\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n","  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n","[Epoch 101/110] [D loss: 1.790643] [G loss: 74.887604] : 100% 36/36 [00:19<00:00,  1.81it/s]\n","[Epoch 102/110] [D loss: 1.822962] [G loss: 74.003020] : 100% 36/36 [00:18<00:00,  1.94it/s]\n","[Epoch 103/110] [D loss: 1.803957] [G loss: 76.455215] : 100% 36/36 [00:18<00:00,  1.93it/s]\n","[Epoch 104/110] [D loss: 1.848151] [G loss: 76.146450] : 100% 36/36 [00:19<00:00,  1.88it/s]\n","[Epoch 105/110] [D loss: 1.910410] [G loss: 75.388486] : 100% 36/36 [00:19<00:00,  1.84it/s]\n","[Epoch 106/110] [D loss: 1.824531] [G loss: 76.709456] : 100% 36/36 [00:18<00:00,  1.92it/s]\n","[Epoch 107/110] [D loss: 1.821065] [G loss: 75.854504] : 100% 36/36 [00:18<00:00,  1.91it/s]\n","[Epoch 108/110] [D loss: 1.760568] [G loss: 78.156176] : 100% 36/36 [00:19<00:00,  1.87it/s]\n","[Epoch 109/110] [D loss: 1.830280] [G loss: 76.271584] : 100% 36/36 [00:18<00:00,  1.92it/s]\n"]}],"source":["# under ./src\n","%cd voice_conversion/src\n","!python preprocess.py --dataset database --test_size 0.1 --eval_size 0.1\n","!python train.py --dataset database --model_name src/pretrained --epoch 100 --n_epochs 10 --decay_epoch 5 --lr 0.0000000001"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYhY20cJiz1f"},"outputs":[],"source":["# to mount gg drive to run on google.colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27967,"status":"ok","timestamp":1701575511352,"user":{"displayName":"Tam Nguyen","userId":"17807644483590929096"},"user_tz":-480},"id":"Qzuri64nyBDz","outputId":"d1c91575-3058-4c51-87ab-8535c66182e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(epoch=109, model_name='pretrained', trg_id='1', src_id=None, wav='database/spkr_2/Ariana-Grande-Positions-(HipHopKit.com)2.wav', wavdir=None, plot=1, n_overlap=4, img_height=128, img_width=128, channels=1, n_downsample=2, dim=32)\n","  0% 0/84 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n","  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n","/content/drive/MyDrive/capstone-voice-style-transfer/voice_conversion/src/models.py:84: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n","  z = Variable(Tensor(np.random.normal(0, 1, mu.shape)))\n","100% 84/84 [00:02<00:00, 33.20it/s]\n","Reconstructing with Griffin Lim...\n"]}],"source":["# inference on a chosen piece of audio - the second piece of positions is chosen here\n","# trg_id specifies the id of the artist to convert vocal to --> 1 here points to Taylor Swift\n","# TODO: inference on the whole song\n","!python inference.py --epoch 109 --model_name pretrained --trg_id 1 --wav \"database/spkr_2/Ariana-Grande-Positions-(HipHopKit.com)2.wav\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10594,"status":"ok","timestamp":1701571966336,"user":{"displayName":"Tam Nguyen","userId":"17807644483590929096"},"user_tz":-480},"id":"t_Nbt3kLluWH","outputId":"1ebec646-b284-401c-d842-503f4cb7b327"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting webrtcvad\n","  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: webrtcvad\n","  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp310-cp310-linux_x86_64.whl size=73464 sha256=858ed8cd3732ccad6e8c1ffe058c4d187c6440b8e37fc5eeab8f3c9308770bf4\n","  Stored in directory: /root/.cache/pip/wheels/2a/2b/84/ac7bacfe8c68a87c1ee3dd3c66818a54c71599abf308e8eb35\n","Successfully built webrtcvad\n","Installing collected packages: webrtcvad\n","Successfully installed webrtcvad-2.0.10\n"]}],"source":["# install dependencies\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMI52s0yD5HzQh3MrGRuoVX","gpuType":"T4","mount_file_id":"1zCjClH--jLW9i-OFCil0LUhinS6ykA3h","provenance":[]},"kernelspec":{"display_name":"Python 3.8.3 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.3"},"vscode":{"interpreter":{"hash":"b8112ee29900838acf48c3545d0267c128dd156ebc94740e915c54a44e2f1f6f"}}},"nbformat":4,"nbformat_minor":0}
